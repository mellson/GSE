\documentclass{sigchi}

% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage{url}      % llt: nicely formatted URLs
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{pdfpages}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}


% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages,
% to give it a fighting chance of not being over-written,
% since its job is to redefine many LaTeX commands.
\usepackage[pdftex]{hyperref}
\hypersetup{
pdftitle={Deciding presence and interruptibility using only a laptop},
pdfauthor={Kristian S. M. Andersen, Anders Bech Mellson and Mads Daniel Christensen},
pdfkeywords={spce, gse, presence, awareness},
bookmarksnumbered,
pdfstartview={FitH},
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black,
breaklinks=true,
}

% create a shortcut to typeset table headings
\newcommand\tabhead[1]{\small\textbf{#1}}


% End of preamble. Here it comes the document.
\begin{document}

\title{Deciding presence and interruptibility using only a laptop}
\numberofauthors{3}
\author{
  \alignauthor Anders Bech Mellson\\
    \email{anbh@itu.dk}\\
  \alignauthor Kristian S. M. Andersen\\
    \email{ksma@itu.dk}\\
  \alignauthor Mads D. Christensen\\
    \email{mdch@itu.dk}\\
}

\maketitle

\begin{abstract}
This paper presents a practical approach on how to get user presence and interruptability information using only a standard laptop.
\end{abstract}

\keywords{
  Global Software Engineering, Presence Awareness, Interruptibility, Social Contracts of Human Interactions
}

\terms{
  Documentation, Theory
}

\section{Introduction}
Globalization is an economical and societal trend that has pushed industries to move from local to global markets.
Working in a global setting requires practitioners to work in distributed arrangements.

Paraphrasing Herbsleb \cite{herbsleb2007}, many of the mechanisms that work correctly in a co-located setting are absent or disrupted in a distributed arrangement.
Different approaches \cite{bly1993media} \cite{fogarty2004myvine} \cite{hincapie2011design} \cite{lai2003myteam} \cite{want1992active} have been investigated to improve the awareness of the working context that a member of a virtual team has; nonetheless, information like the presence of virtual team members, trivial in a co-located setting, represent an interesting open area of investigation.

One fundamental difference between a co-located arrangement and a distributed one is the lack of presence awareness.
The ability to assess how interruptible another user is, becomes very hard when you are not in the same room.
When you are in the near proximity of another user it is fairly easy to assess the interruptibility of that user.
We use the interruptibility assessment to facility behavior that we consider socially acceptable.
If you work in a distributed setting you are dependent on tool support from computer and communication systems to provide availability information about your co-workers.
Today these systems are largely unaware of the social contracts of human interactions.

Research by the Human Computer Interaction Institute at Carnegie Mellon University \cite{fogarty2005predicting} shows that you can detect the interruptibility of a user using sensor inputs.
In their research they find that you can predict interruptibility with sensor inputs at an accuracy of 68\%.
The researchers note that their results could ``motivate the development of systems that use these models to negotiate interruptions at socially appropriate times.''

We will build on their research and be inspired by their models in a practical pursuit of a presence and interruptibility-sharing system.
The work presented in this paper represents two primary contributions.
First, we demonstrate an implementation of a presence and interruptibility-sharing system for global software engineers.
Second, we evaluate the system.
This is done with a test where we compare the accuracy of our availability prediction model againts the accuracy achieved by humans trying to predict availability.

% Sharam: the last paragraph in the introduction section should provide a very quick overview of how the rest of the paper is structured.
We first discover related work in the field of interruptibility and presence-sharing that can inform the design proces of our system.
We then introduce Approximator, our solution for interruptibility sensing and sharing.
We go on to describe and select the sensors used for Approximator.
Finally we evaluate the system against human estimation of interruptibility.

\section{Related work}
Research on using technology to support awareness in a distributed setting has been going on since the early 90’s.
Some of the work \cite{bly1993media} \cite{gaver1992realizing} \cite{mantei1991experiences} tries to keep an instant audiovisual connection between workplaces.
This has the benefit that not only intentional communication is supported, but also social communication since the co-workers is always visible.
But the approach has some drawbacks.
Users can feel self-conscious about the image of themselves being broadcasted.
Not only that but keeping high quality media streams running all day can also be expensive since it consumes a large amount of bandwidth.

\subsection{Self reporting systems}
Distributed teams can use instant messaging (IM) applications to communicate.
Research on IM usage \cite{nardi2000interaction} \cite{handel2002chat} \cite{tang2001connexus} has shown that IM is not only used for chatting, it is also used to negotiate availability.
Most IM systems rely on the user manually setting their status or on simple activity data, such as mouse movement.
This does not always reflect the availability of the user.

\subsection{Context-aware systems}
Fogarty et al. has shown that sensors can be used to extract satisfying accuracy data about a user.
They have used this data to construct a statistical model \cite{fogarty2004examining}.
Later Fogarty et al. goes on to show that it is possible to construct a prediction model for human interruptibility based on simple hardware sensors that is as accurate as humans predicting interruptibility from a video recording of a user working \cite{fogarty2005predicting}.

\subsubsection{MyTeam}
Several systems have tried to build an awareness solution using sensors.
The earliest work dates back to the active badge system \cite{want1992active}.
A similar approach has been tried in the IM system MyTeam \cite{lai2003myteam}, where they use an active badge sensor in combination with the users computer activity.
This solution builds on the premise that success of communications is having prior knowledge about the availability of others before initiating contact.
Lack of this information may explain why over 60\% of business phone calls fail to reach the intended party \cite{whittaker1995rethinking}.
MyTeam differs from other IM systems in that participants can get information about the availability of colleagues even if that user is not running the MyTeam client.
MyTeam uses photos on a colored background to indicate availability.
A drawback of the system is that it takes up a large portion of the users screen. It is also not possible to initiate communication directly from the MyTeam client.

\subsubsection{MyVine}
A system that also uses sensors to determine availability is MyVine\cite{fogarty2004myvine}.
MyVine resembles MyTeam in the way that they both show availability, and also uses continuous values (e.g. 0-100) in means of showing this.
What differs the two systems, is MyVine’s use of holistic aggregated sensors (speech
detection, location information, computer activity, and calendar information) instead of just two sensors(active badge proximity and keyboard and mouse).
The two systems differs in two more ways.
One is that MyVine is symmetric, which means the user has to be online in order to see other users availability.
The other is that MyVine is an almost-always-on system, which allows the users to continuously be shown as available, despite not having to run in the foreground.
MyVine has a problem with misinterpretations of speech as an indication of being unavailable, while users has observed the speech as an indication of availability.

\subsubsection{SenSay}
The SenSay context-aware mobile phone \cite{siewiorek2003sensay} use accelerometer, light, and microphone sensor inputs to determine the availability of the user.
The phone dynamically adapts it’s volume, vibration etc to match the user's current context.
A novel feature about the phone is that the caller can communicate the urgency of their call.
SenSay uses a decision module, which analyzes the sensor input to decide which state the phone should be in.
The system requires the user to carry additional sensors besides the phone, which is not ideal from a user perspective.

\subsubsection{InterruptMe}
In \cite{hincapie2011design} Ramos et al. evaluates the design space of availability-sharing and introduces six new relevant design dimensions for evaluating availability-sharing systems: abstraction, presentation, information delivery, symmetry, obtrusiveness and temporal gradient.
Informed by the evaluation of other systems InterruptMe is presented.
InterruptMe uses implicit inputs from the user to present availability information.
However to get these inputs several external sensors are needed.
This makes the system unable to run on commodity laptops.

\subsubsection{Approximator}
Approximator differs from MyTeam, SenSey, InterruptMe and self-reporting solutions by being a context-aware application that is easy to deploy and works by utilizing only the sensors that can be found within a standard laptop.
MyTeam, SenSay and InterruptMe all are, or rely, on external sensors in order to archive context-awareness.
MyVine on the other hand, utilizes a combination of integrated sensors and contextual information to infer context-awareness.
MyVine requires information such as computer programs associated with work and access to the persons calendar.
In Approximator we rely only on automatic non-parameterized sensors, meaning that the sensors we use does not require prior knowledge of the person using the machine.
Based on this approach we will investigate how Approximator performs in comparison to human judgement using Fogartys method.

\section{Approximator}
Building on the research of Fogarty et al.\cite{fogarty2005predicting} we want to provide a practical user-centric approach to sharing presence and interruptibility information in a Computer Supported Cooperative Work (CSCW) setting.

 We scope our project to using sensors found in commodity laptops.

 We will evaluate the system by replicating the test done by Fogarty et al.
 Our hope is that we can achieve the same 68\% precision rate using only the sensors in scope.

We call our proposed solution ''Approximator''.

\subsection{Presence and Interruptibility}
We define presence as the notion of being in close proximity to the laptop. A user is present if they are close to their machine.

We define interruptibility as the degree to which the user can be disturbed. A user is highly interruptible if they are not doing anything and ready to work. A user is not interruptible if they are deeply engaged in a task requiring their full attention.

\subsection{System Overview}
Approximator consist of three main parts:
\begin{enumerate}
  \item Sensors collecting information about the user.
  \item An infrastructure aggregating and classifying sensor data.
  \item A client showing classified presence and interruptibility information.
\end{enumerate}

The central component of Approximator is the infrastructure.
The infrastructure is a server component that is deployed to a cloud service.
The infrastructure exposes an API that sensors and clients use to communicate with.

A sensor is related to a given user.
One user can have several sensors.
Sensors register themselves with the infrastructure, which returns a specific endpoint for each user.
The sensors sends raw sensors data to this endpoint.
The infrastructure aggregates and interprets the raw sensor data to deduct presence and interruptibility of users.

Client gets the interpreted presence and interruptibility from the infrastructure and visualizes this to the user.

\begin{figure}[H]
  \centering
  \includegraphics[width=\columnwidth]{figures/system_architecture.pdf}
  \caption{System architecture with communication layer.}
  \label{fig:architecture}
\end{figure}

The next sections will cover each part of the solution in detail.

\subsection{Sensors}
To gather data about the users presence and availability we use sensors.
This project has been restricted to only look at sensors which are available in commodity laptops.
We define a commodity laptop to be a device you can buy at any computer store.
The device should be able to run your operating system of choice.
It should have a camera, an input device, a microphone and radios such as WiFi and Bluetooth built-in.

%TODO check the section and make sure we do what we say here.
In this section we will have a look at some of the sensing capabilities of a commodity laptop.
We will discuss the strengths and weaknesses of these sensors.
As well as providing some reasoning for choosing the sensors we have used in our client.

\subsubsection{Keyboard and Mouse}
By using the mouse and keyboard as sensors we collect every mouse movement and keyboard events and send the last recorded activity every second.
With this information we can infer presence due to the fact that there is someone triggering these events.

The issue with using keyboard and mouse as sensors, is that even though they are a strong indicator of presence, we cannot decipher, based on these two sensors alone, who or what this presence is.
We can tie the machine to a specific user, but we can not infer if the machine is being used by a colleague or if a cat decide to rest on the keyboard.

Another issue with mouse and keyboard sensors is that they provide a very weak indicator of non-presence.
When a key is pressed or the mouse moved, we know that someone is interacting with the machine, but the lack of action cannot tell us whether a user is present or non-present.
A user could be talking on the phone, reading, watching a video etc. all in which the user is present, but the mouse and keyboard cannot provide us with this information.

In terms of interruptibility the information from the mouse and keyboard is too ambiguous to deduct whether a user is busy or interruptible.
If the identity of the user sitting in front of the computer could be confirmed, it would still be difficult to determine whether the behavior is an indication of interruptibility.
An approach to determine interruptibility based on these sensors, would involve sending all information to the decision logic, effectively acting as a key logger, and therefore raising security and privacy issues.
% Jeg ved ikke om det er noget vi skal gå i dybden med, eller om det er nok bare at nævne det her.... eller om det overhoved skal nævnes.
% Min overvejelse er om man kunne bruge en vægtet dictionary hvor specifikke ord fik en vægt på hvor arbejds releterede de er og evt kombinere det med en form for pattern matching hvor sprog mønstre blev undersøgt.
This approach will be described later in the decision logic section.

\subsubsection{Microphone}
The use of a microphone as a sensor allows Approximator to sense the presence of a user, even if the user is not directly interacting with the machine.
With more advanced speech recognition algorithms it could even be used to identify the user/users speaking.
This would provide us not only with presence, but also identity, which was a problem with the keyboard and mouse sensors.

The same issue with the detection of non-presence also appears with the microphone, it is not possible to infer non-presence from the lack of sound.
As described with the keyboard and mouse sensors, a user can perform activities that does not trigger the sensor, and thus be present without detection.

Beside the detection of presence, the microphone provides a tool for inferring interruptibility.
According to Fogarty et al\cite{fogarty2005predicting} speaking present a situation of non-interruptibility.

We have chosen to omit the microphone sensor.
There is a high risk of collecting too much noise compared to the signal we need.
The work environment would need to be very quiet for us to use a normal microphone as a reliable sensor.
For this reason we have dropped the microphone sensor.

\subsubsection{Camera}
Like the microphone the camera enables Approximator to sense the presence of a user without the user having to interact with the machine, but differs in the way it does so.
In the setting of a room a microphone can sense if someone is in the room, but it is hard for the microphone to infer where in this room that they are located.

The camera can sense if someone is directly in front of the screen, which supplies a powerful indicator of presence, as well as an indicator of non-presence.
Unlike the other sensors, the camera can sense if a user is sitting in front of the machine, even if the user is not directly interacting with the machine.
But even though the camera can infer non-presence, there are still some issues.
If a user is located in their office but not visible to the camera, the sensor will report non-presence.

The camera could apply face-recognition in order to determine the identity of the user present.
It could also be used to detect actions that the user is executing e.g. entering the room leaving, leaving the room, reading a book etc. that can be used to measure interruptibility.

\subsubsection{Bluetooth and Wi-Fi}
These sensors can detect other devices such as mobile devices or beacons.
If we assume that a user carries his/her mobile device with them, then the bluetooth or wifi beacon from this device can be used to infer if the users proximity to the machine.
Another usage is if the office the user works in have beacons installed at the workplace, ether as Bluetooth or WiFi beacons, then the machine can infer its current symbolic location.
This can be used to infer presence by detecting if the mobile device is nearby.
The issue here is if the user should forget their mobile device, ether nearby the machine or at home.
In these cases the sensor would receive false data i.e. data that provides wrongful information regarding presence or interruptibility.

\subsubsection{Lightsensor}
The light sensor is measuring the ambient light of the environment.
This can be used to sense if the laptop is in a dark or light room.
We have decided against this sensor for now because we think we will only get redundant inputs.
The conclusions we can draw from this sensor is already provided by the other inputs.

\subsubsection{Process monitor}
This sensor is slightly different than the others since it primary purpose is not to sense presence, but interruptibility.
The idea of the process monitor is to supply it with a list of processes (programs) that it should monitor.
This list consists both of work and leisure associated processes, and the process monitor report if any and which program are in the foreground. %and receiving user input.
Through this we obtain the same or weaker notion of presence and non-presence as we did with the keyboard and mouse sensor, but gain a stronger tool to infer interruptibility.

\subsubsection{Geo-location}
This sensor works by looking up the machines registered IP address, or if available uses the machines built-in GPS, and uses this to acquire a geographical address.
Knowing the whereabouts of the machine neither infers presence nor interruptibility on its own.
But it does supply the other sensors and the decision module with useful information regarding the geographical setting of the machine.
The most simple use of this is to detect if the machine currently resides at the office, home, or another location.
But one of the more useful appliances would be in combination with the camera.
If the camera can detect action such as a user entering or leaving the room, this information could tells us whether the a user is in close proximity to the machine, but only that.
Combined with even the coarse location of the machine, Approximator could infer that a user was in a specific office, in close proximity the machine.

\subsubsection{Calendar Sensor}
This sensor can look at the users calendar and get information about events entered here.
We can use these events to help us calculate interruptibility.

\subsection{Infrastructure}
To provide presence and availability information to cloud workers we need a few things.
Firstly we need to gather data input from the working environment.
Secondly we need to aggregate this data and combine it for each user.
Lastly we need to look at the data and through analyzises provide presence and availability information.

All these tasks are combined in our infrastructure which can read input from sensors and analyze this with a decision module.

The work flow of the infrastructure is:
\begin{enumerate}
  \item Gather data
  \item Aggregate data
  \item Analyze data
  \item Provide data
\end{enumerate}

\subsubsection{Design}
The infrastructure is built to support many sensors for each user.
This modelling can lead to a very high amount of sensors connecting to the system.
To support this scalability need we base our system on the actor model by Carl Hewitt \cite{hewitt1973universal}.

Many software applications are written using the object oriented modelling.
The actor model is an alternative way to model computer programs.
In the actor model the unit of work is an actor.
In object oriented programming the unit of work is an object.
Actors communicate by sending messages to each other.
Actors does not share state, making it possible to run actors in parallel and even on different machines.
This makes the actor model a good candidate for building highly scalable applications.

There are several frameworks implementing the actor model, such as Akka \cite{akka}, Project Orleans \cite{orleans} and most famously Erlang \cite{erlang}.
For this project we have chosen Akka because it is Java based.
This supports the technical knowledge in our group.

We use the general programming language Scala \cite{scala} to program our Akka-based infrastructure.


\begin{figure}[H]
  \centering
  \includegraphics[width=\columnwidth]{figures/infrastructure_design.pdf}
  \caption{Infrastructure Design}
  \label{fig:infrastructure}
\end{figure}

It is possible to deploy Akka applications on a server which can run Java.
For this project we have chosen to deploy our system to Azure \cite{azure}, Microsoft's cloud service for developers.
Having the infrastructure running at a cloud provider makes it reachable from the standard internet.
This enables us to build sensors and demonstrators that can run anywhere, as long as they have an internet connection they can reach our infrastructure.

\subsubsection{Data}
Gathering data happens by sensors providing data.
To communicate with our infrastructure a sensor needs to register with the service.
When a sensor registers itself it provides a username.
This username is used to aggregate a group of sensors together around the notion of user.

\subsection{Client}
Windows based user installable application.
The sensor actually combines two parts of the system.
The sensors are built into the client.
This is done for easy of use for the end user.
Merging the two parts makes it possible to get both parts on the users laptop with a single installation.

\subsubsection{Easy of deployment}
We have focused on creating an easy to deploy system.
The infrastructure component can be deployed to any server capable of running the Java Virtual Machine (JVM).
Currently it is possible to get the source code from GitHub\footnote{\url{https://github.com/mofus/GSE/tree/master/Server}}.
The source code requires that Java and SBT is installed.
With the source code on the server it is a simple run command which will compile, fetch dependencies and run the infrastructure.

The client is even easier this is currently a Windows application which can be downloaded and run directly on any newer Windows installation.

The client is also available as source from the GitHub repository should a user wish to rebuild for Mono.

\section{Decision Logic}
We want to identify a software engineer’s interruptibility value using the sensors in a laptop.
To achieve this there are three sensors providing data; keyboard, mouse and camera (face detection).

Raw sensor data does not say much about presence or availability.
The captured sensor data must be analyzed in order to make sense of it.
The infrastructure has a decision module which can calculate interruptibility values from the sensor inputs.

\subsection{Building a model}
A manual classification approach has been used to classify the collected data.
Two test subjects, user A and user B, was recorded in their work environment.
User A showed a clear coalition between patterns in the sensor data and the reported interruptibility.
Three major scenarios was identified by looking at the data.
The three scenarios are listed below.

\begin{itemize}
  \item \textbf{Scenario 1 – Highly Non-Interruptible}
  \begin{itemize}
    \item Identified primarily with the camera sensor
    \item Few data points compared to the other scenarios
    \item Example tasks
    \begin{itemize}
      \item Reading
      \item Conference call
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item \textbf{Scenario 2 – Highly Non-Interruptible}
  \begin{itemize}
    \item Identified primarily with the keyboard sensor
    \item Example tasks
    \begin{itemize}
      \item Writing
      \item Coding
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item \textbf{Scenario 3 – Highly Interruptible}
  \begin{itemize}
    \item Identified primarily with the mouse sensor
    \item Example tasks
    \begin{itemize}
      \item Browsing
      \item Switching context between work sessions
    \end{itemize}
  \end{itemize}
\end{itemize}

User B however did not show any clear indication of patterns that could identify a given scenario.
This could be because software engineers work in different ways.
There is also a chance that user B is an outlier and therefore not representing the generic software engineer.
Maybe it is user A who is an outlier, we do not know.
It would have been much more clear if any of these theories was true if more test subjects had been involved.

\subsection{Scenario detection}
\textbf{Scenario 1} is detected using face detection.
If a user is sitting in front of the computer the face detection will detect the user as being present.
Detection happens with a frontal or profile detection depending on the angle the camera sees the users face.

We try to catch the scenario of a user sitting in front of the computer using it without the mouse and keyboard as main input.
This could be because the user is reading, having a conversation or something similar.

The keyboard sensor identifies \textbf{scenario 2}.
When a user is typing away, it is interpreted as the user is concentrating on work.
This work can be writing, coding or other keyboard heavy tasks.

When a user is moving the mouse a lot, we identify it as \textbf{scenario 3}.
We hypothesize that heavy mouse activity is an indicator for casual activity.
This could be browsing, moving files or simply a switch of context between work sessions.

\section{Evaluation}
In order to compare the performance of Approximator to the result from Forgartys study, the same method for evaluation is followed for during the experiments of this paper.

\subsection{Method}
The experiment was conducted in two stages.
First the two test subjects are recorded in their work environment with a camera, while Approximators sensors collected data.
The second stage was conducted by inviting people from the software development community to a survey where they reviewed the experiment.
At the survey the participants were asked to assess interruptibility for the two test users based on short sequences of video, showing the test users in random selected situations in their work environment.

\subsubsection{Stage One}
The two test subjects was both males, and 25 and 34 years of age.
The first test subject worked for a security company where his primary task was to maintain and develop on the company system.
Since the company also have offices in other countries, this test subject often did parallel development with the out-of-country teams.
He worked in shared offices, where he shared the office with 3-4 other employees, often people assigned to the same project or with a similar skillset.
It was possible to close the door to these offices, but that rarely occurred.
The other test subject owns a company that does software contract work for other businesses.

The test subjects was recorded with a Sony Handycam HDR-CX360 with wide angle lens and was recorded in mp4 with 720x576 format at 25 frames per second.
The original recording was recorded with sound, but the sound was removed for the test.
The camera was placed at an angle that did not allow for the content of their screen to be visible.
The angle did however show as much of the office as possible, while showing the face of the test subject.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/experiment_setup.png}
  \caption{View of experiment setup}
  \label{fig:experiment}
\end{figure}

Approximator was running on a Lenovo T440s Intel Core i5 64-bit processer 8GB RAM with a Windows 7 OS and a Microsoft Surface Pro 3 i5 processer 8GB Ram with Windows 8.1 OS.
Both machines was mounted in an office dock setup connecting the machine to an external screen, keyboard and mouse.
Approximator utilized the keyboard, mouse and camera as sensors, and sent data from these sensors to the infrastructure throughout the experiment.
This data was to used later in our process.
Approximator was specially configured to prompt a visual and audio bell for the test subject at random intervals averaging at two times per hour.
The test subjects were instructed to self-report their own level of interruptibility on a scale of 1 (highly-interruptible) to 5 (highly-non-interruptible) by show of fingers on one hand when prompted.
The test subjects had no way of pausing the application, but could request retroactively to have sequence deleted if necessary.
The prompts where the test subjects was not present was removed from the dataset after the experiment.
If it should happen that the test subjects clearly registered the prompt, but did not signal their interruptibility, due to them being on the phone or engaged in a other activity that required their full attention, then these would be interpreted by us as a highly non-interruptible events.

The experiment was conducted over a period of two workdays from 9 am to 4:30 pm, yielding a total of 30 hours of video and 43 prompts.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/survey_introduction.png}
  \caption{Introduction to the survey.}
  \label{fig:survey_intro}
\end{figure}

\subsubsection{Stage Two}
At this stage an application with the explicit purpose of peer reviewing the collected recordings was created.
People from the software development community were invited to partake in the survey, and did so under supervision from two of the researchers.
When the users started the survey, they were first presented with the text in figure~\ref{fig:survey_intro} and informed to ask the researchers if they had any questions.
Afterwards they were required to enter their demographic information. 
These demographical questions consisted of gender, age, and ethnicity.
This is to ensure the validity of the paper, but also to see if their could be any correlation between demographics and interruptibility assessment.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/webpage_screenshot.png}
  \caption{Screenshot of survey webpage.}
  \label{fig:webpage}
\end{figure}

The user was then presented with 10 sequences of 30 seconds in length.
These were the 30 seconds leading up to the test subject being prompted for self-report.
3 of the 10 sequences were repeated for the purpose of seeing if the user answered randomly, and if more than two sequences deviated more than two points compared to the previous answers, the results from that test person was deemed invalid.
The user could replay the sequence as many times as they wanted until they felt they could give a valid assessment.
Each sequence was played in full length before the user could rate it and proceed to the next.

A total of 19 people participated in the survey.

\subsection{Data analysis}
After the experiment was concluded, the results gathered from Approximator was compared to the test persons, and the users.
This was done by plotting the values into two confusion matrices.
One comparing the answers of user to answers of the test users.
And one comparing the answers of the user to the results of Approximator.
By finding correlations between hits in the test-user/user and Approximator/user results, we could compare how Approximator performs in relation to human judgement, and also compare our findings to those found by Fogarty et al \cite{fogarty2005predicting}

\subsection{Ablation Test}
When the confusion matrices was constructed, we investigated which sensors supplied the infrastructure with information supporting a higher/lower hit rate.
Based on this investigation we performed an ablation test on the sensors to improve our result.
The experiment involving test users was not repeated as the circumstances in which the experiment was conducted had not changed.
Instead we had Approximator generate new interruptibility values based on the historic information provided from the subset of sensors leftover after the ablation test.
This process was repeated until we found the combination of sensors that provided the most accurate information compared to the information given by the user.

\subsection{Results}
Out of the 43 prompts, 35 was used for the review test.
The remaining 8 was not applicable since the test subject was not present at the time of the prompt.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/Interruptibility_values_videotest.png}
  \caption{Frequency of interruptibility values reported by the test subjects, ranging from 1 (highly interruptible to 5 highly non-interruptible).}
  \label{fig:interruptibilityFrequency}
\end{figure}

\section{Discussion}

% Better ways to do experiment
%   Asking test subjects to provide their assesed interruptibility might interrupt them

Many factors affect how people asses human interruptibility as noted in \cite{Avrahami2007}.
These aspects are hard to incorporate in prediction models.
In this paper we use a limited demography of people to asses interruptiblity.
Factors like culture, age and ethnicity may significantly impact the results.

This paper bases its research background and evalutation of the work done in \cite{fogarty2005predicting} but has a number of different factors. This paper and \cite{fogarty2005predicting} are written over 10 years apart. \cite{fogarty2005predicting} relied heavily on factores like closed office doors and talking on the phone. Today open office spaces with are much more common than in 2003 and talking on the phone is less common in favor of other means of communication like email, and chat. More notably \cite{fogarty2005predicting} based their studies on knowledge workers in an University administration department while we based our paper on software developers in a CSCW setting.

We had hoped that both our test subjects would show the same coalition in their data.
This was unfortunately not the case.
If a similar project is to succeed, we need a better algorithm for detecting patterns.
Instead of having a centralized generic algorithm we could have learning algorithms on each users’ machine.
An algorithm which could be tailored directly at a given users’ needs would be preferable.
One approach is to construct an algorithm with a learning period.
In the learning period, the user needs to answer interruptibility questions at random intervals.
This way the algorithm could build up some training data to use for future evaluations.

\subsection{Future Work}
We chose to focus our paper on the sensors available in a modern laptop for the ease of use and deployability.
Most people today also carry smart phones and newer devices such as fitness trackers and smart watches are gaining popularity.
These devices could supply additional useful data to our infrastructure to enhance the prediction of interruptibility by providing more finegrained position and movement.
We leave these devices for future work as they constrain deployability and are not all common case.

\section{Conclusion}
We set out believing that it was possible to detect a software engineer’s interruptibility value using only the sensors in a laptop.
It turned out that it was possible for one of our two test subjects.
This finding opens more questions than it answers. Is our approach still valid have we tested on more subjects?

We conclude that our initial idea is valid.
However, our project did not include enough test subjects to say anything definite about the feasibility across all software engineers.

\section{Acknowledgements}
We would like to thank Paolo Tell for supervising and guiding us throughout the project.
We are also grateful for the great feedback from Thomas Olof Pederson and Shahram Jalaliniya.
Lastly, we would like to thank all the people who helped do our evaluation.

\balance
\bibliographystyle{acm-sigchi}
\bibliography{ubicomp}

\end{document}
